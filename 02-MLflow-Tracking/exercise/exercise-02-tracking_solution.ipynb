{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlflow \n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path \n",
    "import json \n",
    "# TODO: import mlflow\n",
    "import mlflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the tracking URI to your localhost ip http://127.0.0.1:PORT/ \n",
    "# (PORT is usually 5000)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Start a new mlflow run \"mlflow-tracking\" and export its run_id\n",
    "# IMPORTANT: Keep the naming of \"exercise_2_id\", and \"tracking_run_id\" as they are needed later\n",
    "exercise_2_id = mlflow.set_experiment(\"exercise-02\").experiment_id\n",
    "tracking_run_id = mlflow.start_run(run_name =\"mlflow-tracking\", experiment_id=exercise_2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log single parameter\n",
    "# TODO log a learning_rate of 0.01\n",
    "mlflow.log_param(\"learning_rate\", 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log multiple parameters\n",
    "params = {\"epochs\": 20, \"num\": \"sigmoid\"}\n",
    "# TODO: log the above mentioned parameters to mlflow\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_training import __version__\n",
    "# TODO: set an \"environment\" tag to \"dev\" and a \"username\" tag to \"your name\" and version to __version__\n",
    "\n",
    "mlflow.set_tag(\"version\", __version__)\n",
    "mlflow.set_tags({\"environment\": \"dev\", \"username\": \"Florian Krempl\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Metrics\n",
    "# TODO: Log a F-score of 0.7\n",
    "mlflow.log_metric(\"f_score\", 0.7)\n",
    "\n",
    "# TODO Log multiple metrics (accuracy, recall, precision)\n",
    "metrics = {\n",
    "    \"accuracy\": 0.98, \n",
    "    \"recall\": 0.97, \n",
    "    \"precision\": 0.99\n",
    "}\n",
    "\n",
    "mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "# You can also log metrics during an experiment for example loss\n",
    "# imagine you have a training run and every batch produces a loss value \n",
    "loss_values = np.logspace(4, 0, num=100)\n",
    "\n",
    "for batch_num, loss in enumerate(loss_values):\n",
    "    #TODO log the metric for every batch\n",
    "    mlflow.log_metric(\"loss\", loss, step=batch_num)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the active mlflow run (`mlflow-tracking`) in the experiment `exercise-02` and select `loss` in the run overview to observe the model during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your artifact is stored here:  mlflow-artifacts:/264992445669677316/1ec005054aaf41d0a6cdd124526de25c/artifacts\n"
     ]
    }
   ],
   "source": [
    "# Create an example file output/test.txt\n",
    "with TemporaryDirectory() as temp_dir: \n",
    "    file_path = Path(temp_dir) / \"config.json\"\n",
    "\n",
    "    with file_path.open(\"w\") as outfile:\n",
    "        json.dump(params, outfile)\n",
    "    \n",
    "    mlflow.log_artifact(file_path)\n",
    "\n",
    "artifact_uri = mlflow.get_artifact_uri()\n",
    "print(f\"your artifact is stored here: \", artifact_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/27 16:38:43 INFO mlflow.tracking._tracking_service.client: üèÉ View run mlflow-tracking at: http://127.0.0.1:5000/#/experiments/264992445669677316/runs/1ec005054aaf41d0a6cdd124526de25c.\n",
      "2024/08/27 16:38:43 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/264992445669677316.\n"
     ]
    }
   ],
   "source": [
    "# End previous runs\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Look at your run on the UI and check out everything you logged: \n",
    "- Parameters \n",
    "- Tags\n",
    "- Metrics \n",
    "- Artifacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autolog \n",
    "MLFlow also provides a Autolog function for some Frameworks. With SKlearn this works very well if you want just some basic metrics and logs for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/27 16:42:31 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\florian.krempl.AT\\Documents\\projects\\trainings\\mlflow-training\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\"\n",
      "2024/08/27 16:42:31 INFO mlflow.tracking._tracking_service.client: üèÉ View run autologging model example at: http://127.0.0.1:5000/#/experiments/264992445669677316/runs/fbdd4fe2b19b4d56954ac762b9b28950.\n",
      "2024/08/27 16:42:31 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/264992445669677316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: fbdd4fe2b19b4d56954ac762b9b28950\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {\"n_estimators\": 4, \"random_state\": 42}\n",
    "\n",
    "# TODO: start autologging the upcoming run\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "run_name = 'autologging model example'\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    rfr = RandomForestRegressor(**params).fit(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]), np.array([1, 1, 1]))\n",
    "    print(f\"run_id: {run.info.run_id}\")\n",
    "\n",
    "# TODO: stop autologging\n",
    "mlflow.sklearn.autolog(disable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>metrics.accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ec005054aaf41d0a6cdd124526de25c</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3a6e1b4a32a742e28c4dc9f90b0e4aca</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>554b8d4376844382a9843082fd327605</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fbdd4fe2b19b4d56954ac762b9b28950</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>be50375876b147cf83299705401e1c02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id  metrics.accuracy\n",
       "1  1ec005054aaf41d0a6cdd124526de25c              0.98\n",
       "3  3a6e1b4a32a742e28c4dc9f90b0e4aca              0.98\n",
       "4  554b8d4376844382a9843082fd327605              0.98\n",
       "0  fbdd4fe2b19b4d56954ac762b9b28950               NaN\n",
       "2  be50375876b147cf83299705401e1c02               NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO search for runs in our experiment and sort them by accuracy  \n",
    "\n",
    "runs_df = mlflow.search_runs(experiment_names=[\"exercise-02\"])\n",
    "runs_df.sort_values(\"metrics.accuracy\")[[\"run_id\", \"metrics.accuracy\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
